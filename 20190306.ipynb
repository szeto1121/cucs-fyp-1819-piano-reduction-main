{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import piano_reduction.tools as pr\n",
    "import piano_reduction.compute_features as cf\n",
    "from piano_reduction.classes import ScoreData, Reduction\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_gru(features = ['active_rhythm', 'bass_line', 'entrance_effect', 'harmony', 'highest', \n",
    "                                  'in_chord', 'lowest', 'occurrence', 'onset_after_rest', 'rhythm_variety', \n",
    "                                  'strong_beats', 'sustained_rhythm', 'vertical_doubling']):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Activation, GRU, Concatenate, Input, Lambda\n",
    "    from keras.models import Model\n",
    "    input_layer = Input(shape=(21, 128 + 128 * len(features)))\n",
    "    layer = Lambda(lambda x:x[:,:11], output_shape=(11,128 + 128 * len(features)))(input_layer)\n",
    "    layer = GRU(200)(layer)\n",
    "    layer_1 = layer\n",
    "    layer = Lambda(lambda x:x[:,10:], output_shape=(11,128 + 128 * len(features)))(input_layer)\n",
    "    layer = GRU(200, go_backwards=True)(layer)\n",
    "    layer_2 = layer\n",
    "    layer = Concatenate(axis=1)([layer_1, layer_2])\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(128)(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=input_layer, outputs=layer)\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', pr.f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 21, 1792)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 11, 1792)     0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 11, 1792)     0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_27 (GRU)                    (None, 200)          1195800     lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_28 (GRU)                    (None, 200)          1195800     lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 400)          0           gru_27[0][0]                     \n",
      "                                                                 gru_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 400)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 400)          0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 128)          51328       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 128)          0           dense_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,442,928\n",
      "Trainable params: 2,442,928\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bidirectional_gru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ScoreData.load('score_data/cosi_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data.generate_data_5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 21\n",
    "DATE = '20190306b'\n",
    "train_list = [2, 6, 8]\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3,2,1,0\"\n",
    "os.makedirs('models/%s' % DATE, exist_ok=True)\n",
    "\n",
    "import piano_reduction.tools as pr\n",
    "import piano_reduction.compute_features as cf\n",
    "import piano_reduction.keras_models as km\n",
    "from piano_reduction.classes import ScoreData, Reduction\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "features = ['active_rhythm', 'bass_line', 'entrance_effect', 'highest', 'in_chord', 'lowest', 'occurrence', \n",
    "   'onset_after_rest', 'rhythm_variety', 'strong_beats', 'sustained_rhythm', 'vertical_doubling', 'duration_length']\n",
    "    \n",
    "def evaluate(model, testing_data, x_test, y_test, t=0.5):\n",
    "    validation_data = testing_data.copy()\n",
    "    validation_data = validation_data.merge_binary((model.predict(x_test) > t) * 1, skip_features=True)\n",
    "    tmptrain = validation_data.to_binary('y_train')\n",
    "    tmppred = validation_data.to_binary('y_pred')\n",
    "    scores = []\n",
    "    scores.append(pr.jaccard_similarity(tmptrain, tmppred))\n",
    "    scores.append(pr.pitch_class_jaccard_similarity(tmptrain, tmppred))\n",
    "    scores.append(metrics.accuracy_score(validation_data.df['y_train'], validation_data.df['y_pred']))\n",
    "    scores.append(metrics.f1_score(validation_data.df['y_train'], validation_data.df['y_pred']))\n",
    "    scores.append(metrics.roc_auc_score(tmptrain.flatten(), model.predict(x_test).flatten()))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Jaccard Similarity   Ignore Octave        Accuracy        F1 score   roc_auc_score\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "display = 200\n",
    "print('    %30s %20s %15s %15s %15s %15s' % ('', 'Jaccard Similarity', 'Ignore Octave', 'Accuracy', \n",
    "                                             'F1 score', 'roc_auc_score'))\n",
    "avg = defaultdict(lambda : np.zeros(5))\n",
    "models = {}\n",
    "for i in train_list:\n",
    "    models['GRU_2 %d' % i] = bidirectional_gru()\n",
    "    models['GRU_0 with features %d' % i] = km.gru_with_features(features)\n",
    "progress = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30525, 21, 1792) (118, 21, 1792) (30525, 128) (118, 128)\n",
      "step: 2/60 ...  - loss: 0.1043 - acc: 0.9755 - f1: 0.0019 - val_loss: 0.0682 - val_acc: 0.9803 - val_f1: 0.0000e+00\n",
      "step: 4/60 ...  - loss: 0.0500 - acc: 0.9834 - f1: 0.0447 - val_loss: 0.0445 - val_acc: 0.9801 - val_f1: 0.0254\n",
      "step: 6/60 ...  - loss: 0.0400 - acc: 0.9849 - f1: 0.2486 - val_loss: 0.0346 - val_acc: 0.9803 - val_f1: 0.2180\n",
      "step: 8/60 ...  - loss: 0.0349 - acc: 0.9867 - f1: 0.4339 - val_loss: 0.0295 - val_acc: 0.9849 - val_f1: 0.5404\n",
      "step: 10/60 ...  - loss: 0.0317 - acc: 0.9879 - f1: 0.5283 - val_loss: 0.0266 - val_acc: 0.9878 - val_f1: 0.6695\n",
      "step: 12/60 ...  - loss: 0.0293 - acc: 0.9889 - f1: 0.5865 - val_loss: 0.0251 - val_acc: 0.9893 - val_f1: 0.7216\n",
      "step: 14/60 ...  - loss: 0.0275 - acc: 0.9896 - f1: 0.6254 - val_loss: 0.0236 - val_acc: 0.9905 - val_f1: 0.7620\n",
      "step: 16/60 ...  - loss: 0.0259 - acc: 0.9902 - f1: 0.6557 - val_loss: 0.0225 - val_acc: 0.9907 - val_f1: 0.7713\n",
      "step: 18/60 ...  - loss: 0.0245 - acc: 0.9908 - f1: 0.6810 - val_loss: 0.0220 - val_acc: 0.9908 - val_f1: 0.7722\n",
      "step: 20/60 ...  - loss: 0.0234 - acc: 0.9912 - f1: 0.7008 - val_loss: 0.0215 - val_acc: 0.9908 - val_f1: 0.7747\n",
      "step: 22/60 ...  - loss: 0.0224 - acc: 0.9917 - f1: 0.7187 - val_loss: 0.0209 - val_acc: 0.9911 - val_f1: 0.7791\n",
      "step: 24/60 ...  - loss: 0.0214 - acc: 0.9921 - f1: 0.7347 - val_loss: 0.0207 - val_acc: 0.9912 - val_f1: 0.7826\n",
      "step: 26/60 ...  - loss: 0.0206 - acc: 0.9923 - f1: 0.7464 - val_loss: 0.0207 - val_acc: 0.9916 - val_f1: 0.7914\n",
      "step: 28/60 ...  - loss: 0.0198 - acc: 0.9927 - f1: 0.7603 - val_loss: 0.0205 - val_acc: 0.9915 - val_f1: 0.7888\n",
      "step: 30/60 ...  - loss: 0.0192 - acc: 0.9930 - f1: 0.7702 - val_loss: 0.0209 - val_acc: 0.9912 - val_f1: 0.7801\n",
      "step: 32/60 ...  - loss: 0.0185 - acc: 0.9932 - f1: 0.7793 - val_loss: 0.0206 - val_acc: 0.9916 - val_f1: 0.7911\n",
      "step: 34/60 ...  - loss: 0.0179 - acc: 0.9934 - f1: 0.7873 - val_loss: 0.0208 - val_acc: 0.9913 - val_f1: 0.7842\n",
      "step: 36/60 ...  - loss: 0.0173 - acc: 0.9937 - f1: 0.7963 - val_loss: 0.0206 - val_acc: 0.9916 - val_f1: 0.7901\n",
      "step: 38/60 ...  - loss: 0.0168 - acc: 0.9939 - f1: 0.8039 - val_loss: 0.0208 - val_acc: 0.9918 - val_f1: 0.7951\n",
      "step: 40/60 ...  - loss: 0.0163 - acc: 0.9940 - f1: 0.8101 - val_loss: 0.0213 - val_acc: 0.9917 - val_f1: 0.7951\n",
      "step: 42/60 ...  - loss: 0.0158 - acc: 0.9942 - f1: 0.8171 - val_loss: 0.0212 - val_acc: 0.9917 - val_f1: 0.7940\n",
      "step: 44/60 ...  - loss: 0.0154 - acc: 0.9944 - f1: 0.8218 - val_loss: 0.0213 - val_acc: 0.9920 - val_f1: 0.8023\n",
      "step: 46/60 ...  - loss: 0.0150 - acc: 0.9945 - f1: 0.8268 - val_loss: 0.0211 - val_acc: 0.9919 - val_f1: 0.8000\n",
      "step: 48/60 ...  - loss: 0.0146 - acc: 0.9947 - f1: 0.8327 - val_loss: 0.0218 - val_acc: 0.9915 - val_f1: 0.7905\n",
      "step: 50/60 ...  - loss: 0.0142 - acc: 0.9948 - f1: 0.8379 - val_loss: 0.0217 - val_acc: 0.9919 - val_f1: 0.7977\n",
      "step: 52/60 ...  - loss: 0.0138 - acc: 0.9950 - f1: 0.8435 - val_loss: 0.0217 - val_acc: 0.9918 - val_f1: 0.7977\n",
      "step: 54/60 ...  - loss: 0.0135 - acc: 0.9951 - f1: 0.8468 - val_loss: 0.0219 - val_acc: 0.9917 - val_f1: 0.7955\n",
      "step: 56/60 ...  - loss: 0.0132 - acc: 0.9952 - f1: 0.8505 - val_loss: 0.0221 - val_acc: 0.9919 - val_f1: 0.7996\n",
      "step: 58/60 ...  - loss: 0.0130 - acc: 0.9953 - f1: 0.8532 - val_loss: 0.0220 - val_acc: 0.9919 - val_f1: 0.7994\n",
      "step: 60/60 ...  - loss: 0.0127 - acc: 0.9954 - f1: 0.8569 - val_loss: 0.0223 - val_acc: 0.9919 - val_f1: 0.8003\n",
      "  2  GRU_0 with features 2 with 21           0.67397260      0.88442211      0.72768879      0.80523732      0.99286091\n",
      "(30525, 21, 1792) (118, 21, 1792) (30525, 128) (118, 128)\n",
      "step: 2/60 ...  - loss: 0.0824 - acc: 0.9773 - f1: 0.0220 - val_loss: 0.0540 - val_acc: 0.9805 - val_f1: 0.0323\n",
      "step: 4/60 ...  - loss: 0.0389 - acc: 0.9854 - f1: 0.3096 - val_loss: 0.0309 - val_acc: 0.9853 - val_f1: 0.5065\n",
      "step: 6/60 ...  - loss: 0.0315 - acc: 0.9880 - f1: 0.5391 - val_loss: 0.0254 - val_acc: 0.9891 - val_f1: 0.7182\n",
      "step: 8/60 ...  - loss: 0.0276 - acc: 0.9896 - f1: 0.6290 - val_loss: 0.0226 - val_acc: 0.9911 - val_f1: 0.7832\n",
      "step: 10/60 ...  - loss: 0.0249 - acc: 0.9907 - f1: 0.6815 - val_loss: 0.0217 - val_acc: 0.9912 - val_f1: 0.7850\n",
      "step: 12/60 ...  - loss: 0.0227 - acc: 0.9916 - f1: 0.7176 - val_loss: 0.0202 - val_acc: 0.9915 - val_f1: 0.7906\n",
      "step: 14/60 ...  - loss: 0.0210 - acc: 0.9923 - f1: 0.7462 - val_loss: 0.0197 - val_acc: 0.9922 - val_f1: 0.8106\n",
      "step: 16/60 ...  - loss: 0.0195 - acc: 0.9929 - f1: 0.7685 - val_loss: 0.0199 - val_acc: 0.9921 - val_f1: 0.8074\n",
      "step: 18/60 ...  - loss: 0.0182 - acc: 0.9934 - f1: 0.7872 - val_loss: 0.0194 - val_acc: 0.9924 - val_f1: 0.8169\n",
      "step: 20/60 ...  - loss: 0.0170 - acc: 0.9939 - f1: 0.8034 - val_loss: 0.0197 - val_acc: 0.9919 - val_f1: 0.8035\n",
      "step: 22/60 ...  - loss: 0.0160 - acc: 0.9943 - f1: 0.8180 - val_loss: 0.0195 - val_acc: 0.9924 - val_f1: 0.8178\n",
      "step: 24/60 ...  - loss: 0.0151 - acc: 0.9946 - f1: 0.8300 - val_loss: 0.0192 - val_acc: 0.9928 - val_f1: 0.8251\n",
      "step: 26/60 ...  - loss: 0.0142 - acc: 0.9949 - f1: 0.8409 - val_loss: 0.0193 - val_acc: 0.9928 - val_f1: 0.8254\n",
      "step: 28/60 ...  - loss: 0.0133 - acc: 0.9953 - f1: 0.8528 - val_loss: 0.0194 - val_acc: 0.9926 - val_f1: 0.8223\n",
      "step: 30/60 ...  - loss: 0.0127 - acc: 0.9956 - f1: 0.8617 - val_loss: 0.0201 - val_acc: 0.9925 - val_f1: 0.8189\n",
      "step: 32/60 ...  - loss: 0.0120 - acc: 0.9958 - f1: 0.8700 - val_loss: 0.0204 - val_acc: 0.9923 - val_f1: 0.8129\n",
      "step: 34/60 ...  - loss: 0.0113 - acc: 0.9961 - f1: 0.8786 - val_loss: 0.0203 - val_acc: 0.9927 - val_f1: 0.8219\n",
      "step: 36/60 ...  - loss: 0.0106 - acc: 0.9963 - f1: 0.8864 - val_loss: 0.0201 - val_acc: 0.9928 - val_f1: 0.8240\n",
      "step: 38/60 ...  - loss: 0.0101 - acc: 0.9965 - f1: 0.8937 - val_loss: 0.0208 - val_acc: 0.9927 - val_f1: 0.8219\n",
      "step: 40/60 ...  - loss: 0.0096 - acc: 0.9967 - f1: 0.8998 - val_loss: 0.0208 - val_acc: 0.9927 - val_f1: 0.8223\n",
      "step: 42/60 ...  - loss: 0.0091 - acc: 0.9969 - f1: 0.9054 - val_loss: 0.0210 - val_acc: 0.9927 - val_f1: 0.8214\n",
      "  2                GRU_2 2 with 21           0.70879121      0.90954774      0.75743707      0.82958199      0.99699174\n",
      "(30425, 21, 1792) (118, 21, 1792) (30425, 128) (118, 128)\n",
      "step: 2/60 ...  - loss: 0.1072 - acc: 0.9742 - f1: 0.0024 - val_loss: 0.0717 - val_acc: 0.9797 - val_f1: 0.0000e+00\n",
      "step: 4/60 ...  - loss: 0.0514 - acc: 0.9829 - f1: 0.0316 - val_loss: 0.0469 - val_acc: 0.9794 - val_f1: 0.0297\n",
      "step: 6/60 ...  - loss: 0.0406 - acc: 0.9845 - f1: 0.2421 - val_loss: 0.0357 - val_acc: 0.9808 - val_f1: 0.3145\n",
      "step: 8/60 ...  - loss: 0.0353 - acc: 0.9864 - f1: 0.4336 - val_loss: 0.0310 - val_acc: 0.9862 - val_f1: 0.6046\n",
      "step: 10/60 ...  - loss: 0.0321 - acc: 0.9876 - f1: 0.5250 - val_loss: 0.0287 - val_acc: 0.9886 - val_f1: 0.7015\n",
      "step: 12/60 ...  - loss: 0.0300 - acc: 0.9884 - f1: 0.5782 - val_loss: 0.0274 - val_acc: 0.9895 - val_f1: 0.7351\n",
      "step: 14/60 ...  - loss: 0.0282 - acc: 0.9891 - f1: 0.6158 - val_loss: 0.0256 - val_acc: 0.9907 - val_f1: 0.7739\n",
      "step: 16/60 ...  - loss: 0.0268 - acc: 0.9896 - f1: 0.6421 - val_loss: 0.0252 - val_acc: 0.9910 - val_f1: 0.7818\n",
      "step: 18/60 ...  - loss: 0.0255 - acc: 0.9901 - f1: 0.6646 - val_loss: 0.0240 - val_acc: 0.9909 - val_f1: 0.7823\n",
      "step: 20/60 ...  - loss: 0.0244 - acc: 0.9906 - f1: 0.6839 - val_loss: 0.0231 - val_acc: 0.9911 - val_f1: 0.7873\n",
      "step: 22/60 ...  - loss: 0.0235 - acc: 0.9909 - f1: 0.7003 - val_loss: 0.0225 - val_acc: 0.9915 - val_f1: 0.7981\n",
      "step: 24/60 ...  - loss: 0.0226 - acc: 0.9913 - f1: 0.7146 - val_loss: 0.0220 - val_acc: 0.9915 - val_f1: 0.7994\n",
      "step: 26/60 ...  - loss: 0.0217 - acc: 0.9916 - f1: 0.7277 - val_loss: 0.0219 - val_acc: 0.9914 - val_f1: 0.7981\n",
      "step: 28/60 ...  - loss: 0.0210 - acc: 0.9919 - f1: 0.7392 - val_loss: 0.0213 - val_acc: 0.9914 - val_f1: 0.7981\n",
      "step: 30/60 ...  - loss: 0.0203 - acc: 0.9922 - f1: 0.7507 - val_loss: 0.0212 - val_acc: 0.9916 - val_f1: 0.8034\n",
      "step: 32/60 ...  - loss: 0.0197 - acc: 0.9925 - f1: 0.7611 - val_loss: 0.0207 - val_acc: 0.9917 - val_f1: 0.8061\n",
      "step: 34/60 ...  - loss: 0.0191 - acc: 0.9927 - f1: 0.7683 - val_loss: 0.0207 - val_acc: 0.9916 - val_f1: 0.8055\n",
      "step: 36/60 ...  - loss: 0.0186 - acc: 0.9929 - f1: 0.7769 - val_loss: 0.0204 - val_acc: 0.9918 - val_f1: 0.8086\n",
      "step: 38/60 ...  - loss: 0.0180 - acc: 0.9931 - f1: 0.7848 - val_loss: 0.0198 - val_acc: 0.9918 - val_f1: 0.8113\n",
      "step: 40/60 ...  - loss: 0.0176 - acc: 0.9933 - f1: 0.7901 - val_loss: 0.0197 - val_acc: 0.9922 - val_f1: 0.8184\n",
      "step: 42/60 ...  - loss: 0.0171 - acc: 0.9935 - f1: 0.7982 - val_loss: 0.0198 - val_acc: 0.9921 - val_f1: 0.8188\n",
      "step: 44/60 ...  - loss: 0.0166 - acc: 0.9937 - f1: 0.8047 - val_loss: 0.0196 - val_acc: 0.9922 - val_f1: 0.8204\n",
      "step: 46/60 ...  - loss: 0.0163 - acc: 0.9938 - f1: 0.8097 - val_loss: 0.0197 - val_acc: 0.9918 - val_f1: 0.8092\n",
      "step: 48/60 ...  - loss: 0.0159 - acc: 0.9940 - f1: 0.8148 - val_loss: 0.0191 - val_acc: 0.9920 - val_f1: 0.8164\n",
      "step: 50/60 ...  - loss: 0.0155 - acc: 0.9942 - f1: 0.8202 - val_loss: 0.0193 - val_acc: 0.9924 - val_f1: 0.8244\n",
      "step: 52/60 ...  - loss: 0.0152 - acc: 0.9943 - f1: 0.8238 - val_loss: 0.0190 - val_acc: 0.9922 - val_f1: 0.8210\n",
      "step: 54/60 ...  - loss: 0.0149 - acc: 0.9944 - f1: 0.8277 - val_loss: 0.0189 - val_acc: 0.9922 - val_f1: 0.8213\n",
      "step: 56/60 ...  - loss: 0.0146 - acc: 0.9945 - f1: 0.8324 - val_loss: 0.0190 - val_acc: 0.9921 - val_f1: 0.8165\n",
      "step: 58/60 ...  - loss: 0.0142 - acc: 0.9947 - f1: 0.8370 - val_loss: 0.0186 - val_acc: 0.9923 - val_f1: 0.8215\n",
      "step: 60/60 ...  - loss: 0.0139 - acc: 0.9948 - f1: 0.8401 - val_loss: 0.0189 - val_acc: 0.9923 - val_f1: 0.8217\n",
      "  6  GRU_0 with features 6 with 21           0.70052083      0.92385787      0.74099099      0.82388974      0.99696170\n",
      "(30425, 21, 1792) (118, 21, 1792) (30425, 128) (118, 128)\n",
      "step: 2/60 ...  - loss: 0.0841 - acc: 0.9767 - f1: 0.0149 - val_loss: 0.0565 - val_acc: 0.9797 - val_f1: 0.0219\n",
      "step: 4/60 ...  - loss: 0.0396 - acc: 0.9851 - f1: 0.3058 - val_loss: 0.0336 - val_acc: 0.9855 - val_f1: 0.5643\n",
      "step: 6/60 ...  - loss: 0.0320 - acc: 0.9877 - f1: 0.5365 - val_loss: 0.0284 - val_acc: 0.9893 - val_f1: 0.7333\n",
      "step: 8/60 ...  - loss: 0.0282 - acc: 0.9891 - f1: 0.6219 - val_loss: 0.0255 - val_acc: 0.9906 - val_f1: 0.7749\n",
      "step: 10/60 ...  - loss: 0.0257 - acc: 0.9901 - f1: 0.6670 - val_loss: 0.0251 - val_acc: 0.9907 - val_f1: 0.7833\n",
      "step: 12/60 ...  - loss: 0.0237 - acc: 0.9909 - f1: 0.7012 - val_loss: 0.0244 - val_acc: 0.9909 - val_f1: 0.7893\n",
      "step: 14/60 ...  - loss: 0.0219 - acc: 0.9916 - f1: 0.7299 - val_loss: 0.0233 - val_acc: 0.9912 - val_f1: 0.7941\n",
      "step: 16/60 ...  - loss: 0.0205 - acc: 0.9923 - f1: 0.7536 - val_loss: 0.0224 - val_acc: 0.9912 - val_f1: 0.7960\n",
      "step: 18/60 ...  - loss: 0.0192 - acc: 0.9927 - f1: 0.7712 - val_loss: 0.0220 - val_acc: 0.9913 - val_f1: 0.7991\n",
      "step: 20/60 ...  - loss: 0.0181 - acc: 0.9932 - f1: 0.7880 - val_loss: 0.0211 - val_acc: 0.9914 - val_f1: 0.7994\n",
      "step: 22/60 ...  - loss: 0.0170 - acc: 0.9937 - f1: 0.8038 - val_loss: 0.0210 - val_acc: 0.9915 - val_f1: 0.8024\n",
      "step: 24/60 ...  - loss: 0.0161 - acc: 0.9940 - f1: 0.8159 - val_loss: 0.0208 - val_acc: 0.9914 - val_f1: 0.8000\n",
      "step: 26/60 ...  - loss: 0.0153 - acc: 0.9944 - f1: 0.8278 - val_loss: 0.0207 - val_acc: 0.9912 - val_f1: 0.7969\n",
      "step: 28/60 ...  - loss: 0.0145 - acc: 0.9947 - f1: 0.8377 - val_loss: 0.0202 - val_acc: 0.9916 - val_f1: 0.8028\n",
      "step: 30/60 ...  - loss: 0.0138 - acc: 0.9950 - f1: 0.8471 - val_loss: 0.0203 - val_acc: 0.9914 - val_f1: 0.7978\n",
      "step: 32/60 ...  - loss: 0.0131 - acc: 0.9953 - f1: 0.8576 - val_loss: 0.0209 - val_acc: 0.9914 - val_f1: 0.8012\n",
      "step: 34/60 ...  - loss: 0.0125 - acc: 0.9955 - f1: 0.8649 - val_loss: 0.0204 - val_acc: 0.9913 - val_f1: 0.7991\n",
      "step: 36/60 ...  - loss: 0.0118 - acc: 0.9958 - f1: 0.8722 - val_loss: 0.0205 - val_acc: 0.9914 - val_f1: 0.8022\n",
      "step: 38/60 ...  - loss: 0.0113 - acc: 0.9960 - f1: 0.8791 - val_loss: 0.0218 - val_acc: 0.9914 - val_f1: 0.8009\n",
      "step: 40/60 ...  - loss: 0.0107 - acc: 0.9962 - f1: 0.8858 - val_loss: 0.0197 - val_acc: 0.9920 - val_f1: 0.8121\n",
      "step: 42/60 ...  - loss: 0.0102 - acc: 0.9964 - f1: 0.8922 - val_loss: 0.0208 - val_acc: 0.9917 - val_f1: 0.8065\n",
      "step: 44/60 ...  - loss: 0.0098 - acc: 0.9966 - f1: 0.8983 - val_loss: 0.0208 - val_acc: 0.9920 - val_f1: 0.8121\n",
      "step: 46/60 ...  - loss: 0.0093 - acc: 0.9968 - f1: 0.9035 - val_loss: 0.0216 - val_acc: 0.9919 - val_f1: 0.8119\n",
      "step: 48/60 ...  - loss: 0.0089 - acc: 0.9969 - f1: 0.9086 - val_loss: 0.0212 - val_acc: 0.9922 - val_f1: 0.8182\n",
      "step: 50/60 ...  - loss: 0.0085 - acc: 0.9970 - f1: 0.9122 - val_loss: 0.0216 - val_acc: 0.9919 - val_f1: 0.8114\n",
      "step: 52/60 ...  - loss: 0.0082 - acc: 0.9972 - f1: 0.9167 - val_loss: 0.0208 - val_acc: 0.9923 - val_f1: 0.8207\n",
      "step: 54/60 ...  - loss: 0.0078 - acc: 0.9973 - f1: 0.9206 - val_loss: 0.0211 - val_acc: 0.9922 - val_f1: 0.8176\n",
      "step: 56/60 ...  - loss: 0.0075 - acc: 0.9974 - f1: 0.9247 - val_loss: 0.0215 - val_acc: 0.9920 - val_f1: 0.8112\n",
      "step: 58/60 ...  - loss: 0.0072 - acc: 0.9975 - f1: 0.9278 - val_loss: 0.0219 - val_acc: 0.9919 - val_f1: 0.8093\n",
      "step: 60/60 ...  - loss: 0.0069 - acc: 0.9977 - f1: 0.9318 - val_loss: 0.0215 - val_acc: 0.9922 - val_f1: 0.8150\n",
      "  6                GRU_2 6 with 21           0.69028871      0.94897959      0.73423423      0.81677019      0.99663640\n",
      "(30550, 21, 1792) (118, 21, 1792) (30550, 128) (118, 128)\n",
      "step: 2/60 ...  - loss: 0.0973 - acc: 0.9779 - f1: 0.0030 - val_loss: 0.0634 - val_acc: 0.9848 - val_f1: 0.0000e+00\n",
      "step: 4/60 ...  - loss: 0.0457 - acc: 0.9867 - f1: 0.0044 - val_loss: 0.0461 - val_acc: 0.9848 - val_f1: 0.0000e+00\n",
      "step: 6/60 ...  - loss: 0.0363 - acc: 0.9871 - f1: 0.0843 - val_loss: 0.0378 - val_acc: 0.9840 - val_f1: 0.0620\n",
      "step: 8/60 ...  - loss: 0.0314 - acc: 0.9879 - f1: 0.2515 - val_loss: 0.0336 - val_acc: 0.9849 - val_f1: 0.3188\n",
      "step: 10/60 ...  - loss: 0.0286 - acc: 0.9888 - f1: 0.3764 - val_loss: 0.0312 - val_acc: 0.9860 - val_f1: 0.4597\n",
      "step: 12/60 ...  - loss: 0.0265 - acc: 0.9895 - f1: 0.4538 - val_loss: 0.0301 - val_acc: 0.9858 - val_f1: 0.4873\n",
      "step: 14/60 ...  - loss: 0.0249 - acc: 0.9901 - f1: 0.5061 - val_loss: 0.0292 - val_acc: 0.9858 - val_f1: 0.5142\n",
      "step: 16/60 ...  - loss: 0.0236 - acc: 0.9906 - f1: 0.5489 - val_loss: 0.0285 - val_acc: 0.9858 - val_f1: 0.5258\n",
      "step: 18/60 ...  - loss: 0.0224 - acc: 0.9911 - f1: 0.5817 - val_loss: 0.0278 - val_acc: 0.9862 - val_f1: 0.5486\n",
      "step: 20/60 ...  - loss: 0.0214 - acc: 0.9915 - f1: 0.6101 - val_loss: 0.0277 - val_acc: 0.9866 - val_f1: 0.5504\n",
      "step: 22/60 ...  - loss: 0.0204 - acc: 0.9920 - f1: 0.6364 - val_loss: 0.0273 - val_acc: 0.9874 - val_f1: 0.5809\n",
      "step: 24/60 ...  - loss: 0.0195 - acc: 0.9923 - f1: 0.6590 - val_loss: 0.0273 - val_acc: 0.9871 - val_f1: 0.5770\n",
      "step: 26/60 ...  - loss: 0.0188 - acc: 0.9927 - f1: 0.6769 - val_loss: 0.0270 - val_acc: 0.9879 - val_f1: 0.6002\n",
      "step: 28/60 ...  - loss: 0.0181 - acc: 0.9930 - f1: 0.6944 - val_loss: 0.0270 - val_acc: 0.9877 - val_f1: 0.5920\n",
      "step: 30/60 ...  - loss: 0.0174 - acc: 0.9933 - f1: 0.7097 - val_loss: 0.0264 - val_acc: 0.9881 - val_f1: 0.6062\n",
      "step: 32/60 ...  - loss: 0.0167 - acc: 0.9936 - f1: 0.7246 - val_loss: 0.0266 - val_acc: 0.9880 - val_f1: 0.6065\n",
      "step: 34/60 ...  - loss: 0.0162 - acc: 0.9938 - f1: 0.7359 - val_loss: 0.0267 - val_acc: 0.9884 - val_f1: 0.6202\n",
      "step: 36/60 ...  - loss: 0.0157 - acc: 0.9940 - f1: 0.7472 - val_loss: 0.0269 - val_acc: 0.9883 - val_f1: 0.6217\n",
      "step: 38/60 ...  - loss: 0.0152 - acc: 0.9942 - f1: 0.7578 - val_loss: 0.0269 - val_acc: 0.9887 - val_f1: 0.6315\n",
      "step: 40/60 ...  - loss: 0.0147 - acc: 0.9944 - f1: 0.7683 - val_loss: 0.0274 - val_acc: 0.9883 - val_f1: 0.6225\n",
      "step: 42/60 ...  - loss: 0.0144 - acc: 0.9946 - f1: 0.7751 - val_loss: 0.0270 - val_acc: 0.9886 - val_f1: 0.6304\n",
      "step: 44/60 ...  - loss: 0.0139 - acc: 0.9948 - f1: 0.7846 - val_loss: 0.0274 - val_acc: 0.9883 - val_f1: 0.6210\n",
      "step: 46/60 ...  - loss: 0.0136 - acc: 0.9949 - f1: 0.7917 - val_loss: 0.0277 - val_acc: 0.9878 - val_f1: 0.6062\n",
      "step: 48/60 ...  - loss: 0.0132 - acc: 0.9951 - f1: 0.7987 - val_loss: 0.0280 - val_acc: 0.9882 - val_f1: 0.6197\n",
      "step: 50/60 ...  - loss: 0.0129 - acc: 0.9952 - f1: 0.8031 - val_loss: 0.0278 - val_acc: 0.9881 - val_f1: 0.6191\n",
      "step: 52/60 ...  - loss: 0.0126 - acc: 0.9953 - f1: 0.8089 - val_loss: 0.0279 - val_acc: 0.9881 - val_f1: 0.6152\n",
      "step: 54/60 ...  - loss: 0.0123 - acc: 0.9954 - f1: 0.8149 - val_loss: 0.0283 - val_acc: 0.9880 - val_f1: 0.6147\n",
      "step: 56/60 ...  - loss: 0.0120 - acc: 0.9955 - f1: 0.8182 - val_loss: 0.0281 - val_acc: 0.9885 - val_f1: 0.6282\n",
      "step: 58/60 ...  - loss: 0.0117 - acc: 0.9956 - f1: 0.8232 - val_loss: 0.0284 - val_acc: 0.9884 - val_f1: 0.6253\n",
      "step: 60/60 ...  - loss: 0.0115 - acc: 0.9957 - f1: 0.8280 - val_loss: 0.0286 - val_acc: 0.9887 - val_f1: 0.6345\n",
      "  8  GRU_0 with features 8 with 21           0.46226415      0.81168831      0.63383298      0.63225806      0.99294772\n",
      "(30550, 21, 1792) (118, 21, 1792) (30550, 128) (118, 128)\n",
      "step: 2/60 ...  - loss: 0.0752 - acc: 0.9808 - f1: 0.0033 - val_loss: 0.0538 - val_acc: 0.9848 - val_f1: 0.0000e+00\n",
      "step: 4/60 ...  - loss: 0.0357 - acc: 0.9873 - f1: 0.1444 - val_loss: 0.0350 - val_acc: 0.9853 - val_f1: 0.2218\n",
      "step: 6/60 ...  - loss: 0.0285 - acc: 0.9889 - f1: 0.3877 - val_loss: 0.0311 - val_acc: 0.9859 - val_f1: 0.4897\n",
      "step: 8/60 ...  - loss: 0.0249 - acc: 0.9901 - f1: 0.5157 - val_loss: 0.0293 - val_acc: 0.9861 - val_f1: 0.5456\n",
      "step: 10/60 ...  - loss: 0.0224 - acc: 0.9911 - f1: 0.5856 - val_loss: 0.0289 - val_acc: 0.9861 - val_f1: 0.5641\n",
      "step: 12/60 ...  - loss: 0.0204 - acc: 0.9920 - f1: 0.6397 - val_loss: 0.0281 - val_acc: 0.9866 - val_f1: 0.5826\n",
      "step: 14/60 ...  - loss: 0.0187 - acc: 0.9927 - f1: 0.6803 - val_loss: 0.0280 - val_acc: 0.9868 - val_f1: 0.5899\n",
      "step: 16/60 ...  - loss: 0.0172 - acc: 0.9934 - f1: 0.7186 - val_loss: 0.0283 - val_acc: 0.9872 - val_f1: 0.6080\n",
      "step: 18/60 ...  - loss: 0.0159 - acc: 0.9940 - f1: 0.7462 - val_loss: 0.0286 - val_acc: 0.9873 - val_f1: 0.6104\n",
      "step: 20/60 ...  - loss: 0.0148 - acc: 0.9945 - f1: 0.7717 - val_loss: 0.0286 - val_acc: 0.9878 - val_f1: 0.6201\n",
      "step: 22/60 ...  - loss: 0.0138 - acc: 0.9949 - f1: 0.7905 - val_loss: 0.0291 - val_acc: 0.9880 - val_f1: 0.6303\n",
      "step: 24/60 ...  - loss: 0.0129 - acc: 0.9953 - f1: 0.8090 - val_loss: 0.0296 - val_acc: 0.9879 - val_f1: 0.6296\n",
      "step: 26/60 ...  - loss: 0.0121 - acc: 0.9956 - f1: 0.8240 - val_loss: 0.0286 - val_acc: 0.9886 - val_f1: 0.6432\n",
      "step: 28/60 ...  - loss: 0.0114 - acc: 0.9959 - f1: 0.8360 - val_loss: 0.0289 - val_acc: 0.9883 - val_f1: 0.6386\n",
      "step: 30/60 ...  - loss: 0.0107 - acc: 0.9962 - f1: 0.8488 - val_loss: 0.0301 - val_acc: 0.9882 - val_f1: 0.6389\n",
      "step: 32/60 ...  - loss: 0.0101 - acc: 0.9964 - f1: 0.8583 - val_loss: 0.0302 - val_acc: 0.9885 - val_f1: 0.6504\n",
      "step: 34/60 ...  - loss: 0.0096 - acc: 0.9966 - f1: 0.8668 - val_loss: 0.0299 - val_acc: 0.9891 - val_f1: 0.6612\n",
      "step: 36/60 ...  - loss: 0.0091 - acc: 0.9969 - f1: 0.8767 - val_loss: 0.0315 - val_acc: 0.9882 - val_f1: 0.6446\n",
      "step: 38/60 ...  - loss: 0.0086 - acc: 0.9970 - f1: 0.8836 - val_loss: 0.0315 - val_acc: 0.9889 - val_f1: 0.6579\n",
      "step: 40/60 ...  - loss: 0.0082 - acc: 0.9972 - f1: 0.8909 - val_loss: 0.0325 - val_acc: 0.9884 - val_f1: 0.6528\n",
      "step: 42/60 ...  - loss: 0.0078 - acc: 0.9974 - f1: 0.8969 - val_loss: 0.0329 - val_acc: 0.9886 - val_f1: 0.6567\n",
      "step: 44/60 ...  - loss: 0.0074 - acc: 0.9975 - f1: 0.9023 - val_loss: 0.0328 - val_acc: 0.9886 - val_f1: 0.6525\n",
      "step: 46/60 ...  - loss: 0.0070 - acc: 0.9977 - f1: 0.9089 - val_loss: 0.0331 - val_acc: 0.9890 - val_f1: 0.6585\n",
      "step: 48/60 ...  - loss: 0.0067 - acc: 0.9978 - f1: 0.9142 - val_loss: 0.0331 - val_acc: 0.9891 - val_f1: 0.6598\n",
      "step: 50/60 ...  - loss: 0.0064 - acc: 0.9979 - f1: 0.9174 - val_loss: 0.0341 - val_acc: 0.9887 - val_f1: 0.6545\n",
      "step: 52/60 ...  - loss: 0.0061 - acc: 0.9980 - f1: 0.9216 - val_loss: 0.0343 - val_acc: 0.9888 - val_f1: 0.6572\n",
      "step: 54/60 ...  - loss: 0.0059 - acc: 0.9981 - f1: 0.9261 - val_loss: 0.0345 - val_acc: 0.9890 - val_f1: 0.6591\n",
      "step: 56/60 ...  - loss: 0.0056 - acc: 0.9982 - f1: 0.9290 - val_loss: 0.0352 - val_acc: 0.9886 - val_f1: 0.6539\n",
      "step: 58/60 ...  - loss: 0.0054 - acc: 0.9982 - f1: 0.9325 - val_loss: 0.0360 - val_acc: 0.9887 - val_f1: 0.6546\n",
      "step: 60/60 ...  - loss: 0.0052 - acc: 0.9983 - f1: 0.9358 - val_loss: 0.0362 - val_acc: 0.9886 - val_f1: 0.6490\n",
      "  8                GRU_2 8 with 21           0.47575758      0.80891720      0.63191489      0.64476386      0.99060602\n"
     ]
    }
   ],
   "source": [
    "for i in train_list:\n",
    "    data = ScoreData.load('score_data/cosi_%d.pkl' % i)\n",
    "    \n",
    "    training_data, testing_data = data.split(0.9)\n",
    "    \n",
    "    name = 'GRU_0 with features %d' % i\n",
    "    model = models[name]\n",
    "    length = LENGTH\n",
    "    logger = pr.NBatchLogger(display=2)\n",
    "    x_train, y_train = None, None\n",
    "    x_test, y_test = testing_data.generate_data_4(length=length, features=features)\n",
    "    data_tmp = training_data.copy()\n",
    "    for key in range(-12, 13):\n",
    "        data_tmp.df['ps'] = data.df['ps'] + key\n",
    "        x_tmp, y_tmp = data_tmp.generate_data_4(length=length, features=features)\n",
    "        if key == -12:\n",
    "            x_train, y_train = x_tmp, y_tmp\n",
    "        else:\n",
    "            x_train, y_train = np.append(x_train, x_tmp, axis=0), np.append(y_train, y_tmp, axis=0)\n",
    "\n",
    "    early_stop = pr.early_stop(patience=20)\n",
    "    chunk = x_train.shape[0] // 1\n",
    "    for j in range(0, x_train.shape[0], chunk):\n",
    "        print(x_train[j : j + chunk].shape, x_test.shape, y_train[j : j + chunk].shape, y_test.shape)\n",
    "        model.fit(x_train[j : j + chunk], y_train[j : j + chunk], epochs=60, batch_size=512, validation_data=(x_test, y_test), callbacks=[logger, early_stop], verbose=0)\n",
    "    scores = evaluate(model, testing_data, x_test, y_test)\n",
    "    avg['%s with %d' % (name, length)] += np.array(scores)\n",
    "    models[name] = model\n",
    "    print('%3d %30s %20.8f %15.8f %15.8f %15.8f %15.8f' % tuple([i] + ['%s with %d' % (name, length)] + scores))\n",
    "    progress = '%d %d %s' % (i, 0, name)\n",
    "    with open('log_slurm.txt', 'a') as f:\n",
    "        f.write(progress + '\\n')\n",
    "        f.write('%s\\n' % ('%3d %30s %20.8f %15.8f %15.8f %15.8f %15.8f' % tuple([i] + ['%s with %d' % (name, length)] + scores)))\n",
    "    models[name].save('models/%s/%s.h5' % (DATE, name))\n",
    "    red = Reduction('models/%s/%s.h5' % (DATE, name), ScoreData.generate_data_4, {'length' : LENGTH, 'features' : features}, 0.1)\n",
    "    red.save('models/%s/%s.pkl' % (DATE, name))\n",
    "    \n",
    "    name = 'GRU_2 %d' % i\n",
    "    model = models[name]\n",
    "    length = LENGTH\n",
    "    logger = pr.NBatchLogger(display=2)\n",
    "    x_train, y_train = None, None\n",
    "    x_test, y_test = testing_data.generate_data_5(length=length, features=features)\n",
    "    data_tmp = training_data.copy()\n",
    "    for key in range(-12, 13):\n",
    "        data_tmp.df['ps'] = data.df['ps'] + key\n",
    "        x_tmp, y_tmp = data_tmp.generate_data_5(length=length, features=features)\n",
    "        if key == -12:\n",
    "            x_train, y_train = x_tmp, y_tmp\n",
    "        else:\n",
    "            x_train, y_train = np.append(x_train, x_tmp, axis=0), np.append(y_train, y_tmp, axis=0)\n",
    "\n",
    "    early_stop = pr.early_stop(patience=20)\n",
    "    chunk = x_train.shape[0] // 1\n",
    "    for j in range(0, x_train.shape[0], chunk):\n",
    "        print(x_train[j : j + chunk].shape, x_test.shape, y_train[j : j + chunk].shape, y_test.shape)\n",
    "        model.fit(x_train[j : j + chunk], y_train[j : j + chunk], epochs=60, batch_size=512, validation_data=(x_test, y_test), callbacks=[logger, early_stop], verbose=0)\n",
    "    scores = evaluate(model, testing_data, x_test, y_test)\n",
    "    avg['%s with %d' % (name, length)] += np.array(scores)\n",
    "    models[name] = model\n",
    "    print('%3d %30s %20.8f %15.8f %15.8f %15.8f %15.8f' % tuple([i] + ['%s with %d' % (name, length)] + scores))\n",
    "    progress = '%d %d %s' % (i, 0, name)\n",
    "    with open('log_slurm.txt', 'a') as f:\n",
    "        f.write(progress + '\\n')\n",
    "        f.write('%s\\n' % ('%3d %30s %20.8f %15.8f %15.8f %15.8f %15.8f' % tuple([i] + ['%s with %d' % (name, length)] + scores)))\n",
    "    models[name].save('models/%s/%s.h5' % (DATE, name))\n",
    "    red = Reduction('models/%s/%s.h5' % (DATE, name), ScoreData.generate_data_5, {'length' : LENGTH, 'features' : features}, 0.1)\n",
    "    red.save('models/%s/%s.pkl' % (DATE, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
